{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy comunication compression scheme in FedLab\n",
    "\n",
    "This tutorial provides comprehensive examples about implementing a communication efficiency scheme in FedLab. \n",
    "\n",
    "We take the baseline gradient compression algorithms as examples (top-k for gradient sparsification and QSGD for gradient quantization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlab.contrib.compressor.quantization import QSGDCompressor\n",
    "from fedlab.contrib.compressor.topk import TopkCompressor\n",
    "import torch\n",
    "\n",
    "tpk_compressor = TopkCompressor(compress_ratio=0.05) # top 5% gradient \n",
    "qsgd_compressor = QSGDCompressor(n_bit=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be compressed tensor: tensor([ 1.0957, -0.0225,  2.2943, -0.8359, -0.2672, -0.4585, -1.9813,  0.8037,\n",
      "        -1.2134,  1.6239, -2.0395,  0.1773, -0.4373,  0.3860,  0.4555, -0.3621,\n",
      "         0.8813, -2.7425,  0.1360,  0.3829,  0.3990,  2.5764, -0.0890,  0.2678,\n",
      "        -2.1603,  1.3459,  0.3746, -0.2256, -1.2713,  1.3135, -0.6275,  0.1372,\n",
      "        -0.8015, -0.8266, -2.0506, -1.1223,  0.0429,  0.4880, -1.1994,  0.2747,\n",
      "         0.5410,  0.2719, -0.0929,  0.6213, -0.3104,  0.2568, -1.1598,  1.4344,\n",
      "        -0.2909,  0.5093,  0.4411,  0.0876,  0.9613,  3.3478, -0.5169, -0.8202,\n",
      "         1.1259,  1.2786,  0.3161,  1.8539, -1.1810, -0.7764,  1.4986,  1.9026,\n",
      "        -1.7616,  0.2152, -0.9130,  1.0844,  0.1899, -0.3537,  0.9423, -1.2363,\n",
      "        -0.4125, -0.5371, -0.7780, -0.7829, -0.3591, -0.5742,  1.8148,  1.3841,\n",
      "         1.4880,  0.2876, -0.0828,  0.5254,  0.5589, -0.3416,  0.6386,  0.2445,\n",
      "         1.1237, -0.1225, -0.0981, -0.4455,  0.9118,  1.1830, -0.2265, -0.7181,\n",
      "         1.1139,  0.1481,  1.1955, -0.6536])\n",
      "Compressed results top-k values: tensor([ 2.5764,  3.3478, -2.7425,  2.2943, -2.1603])\n",
      "Compressed results top-k indices: tensor([21, 53, 17,  2, 24])\n",
      "Decompressed results: tensor([ 0.0000,  0.0000,  2.2943,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -2.7425,  0.0000,  0.0000,  0.0000,  2.5764,  0.0000,  0.0000,\n",
      "        -2.1603,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.3478,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "# top-k\n",
    "tensor = torch.randn(size=(100,))\n",
    "shape = tensor.shape\n",
    "print(\"To be compressed tensor:\", tensor)\n",
    "\n",
    "# compress\n",
    "values, indices = tpk_compressor.compress(tensor)\n",
    "print(\"Compressed results top-k values:\",values)\n",
    "print(\"Compressed results top-k indices:\", indices)\n",
    "\n",
    "# decompress\n",
    "decompressed = tpk_compressor.decompress(values, indices, shape)\n",
    "print(\"Decompressed results:\", decompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be compressed tensor: tensor([ 0.4980,  1.5365, -1.7806, -0.1770, -0.7686,  0.2866, -0.3974,  0.2999,\n",
      "         1.5777,  1.8726, -0.6649, -0.2412, -2.2352,  0.3977,  2.2475,  0.7586,\n",
      "         1.1875,  0.9025, -0.6957, -1.0138,  0.6565, -2.3868, -0.3719,  0.9737,\n",
      "         0.7330,  2.0682,  0.8694, -0.0777,  0.4824, -1.2656,  0.3540, -0.2026,\n",
      "        -0.7270,  0.6343, -0.1980, -0.1527, -0.7506,  1.8442,  1.1878, -1.5449,\n",
      "         0.0272, -0.8650,  1.8941, -0.8262, -1.1485,  0.3952, -0.0212,  0.4066,\n",
      "         0.2091,  0.4729,  0.2329,  0.0989,  0.2501, -0.5962, -1.1444, -2.2606,\n",
      "        -0.7984,  1.5544,  0.9907,  0.0788, -1.1783,  0.8196, -1.0843,  0.4282,\n",
      "         0.6750, -0.5112,  0.6044, -1.6416, -0.8934, -0.7939, -0.1339, -1.7930,\n",
      "         0.0994, -0.0744,  0.5451, -0.4144, -0.0221,  0.9689, -1.3286, -0.4493,\n",
      "        -0.5176, -0.5471,  1.2051, -0.5761,  0.5324, -2.5486, -1.2086,  1.5447,\n",
      "        -0.0366, -0.9045, -0.7475,  0.2635,  0.3905,  0.7301,  1.0756, -1.3421,\n",
      "         0.1725, -1.7543, -1.3895,  0.0714])\n",
      "Compressed results QSGD norm: tensor([2.5486])\n",
      "Compressed results QSGD signs: tensor([ True,  True, False, False, False,  True, False,  True,  True,  True,\n",
      "        False, False, False,  True,  True,  True,  True,  True, False, False,\n",
      "         True, False, False,  True,  True,  True,  True, False,  True, False,\n",
      "         True, False, False,  True, False, False, False,  True,  True, False,\n",
      "         True, False,  True, False, False,  True, False,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False, False,  True,  True,  True,\n",
      "        False,  True, False,  True,  True, False,  True, False, False, False,\n",
      "        False, False,  True, False,  True, False, False,  True, False, False,\n",
      "        False, False,  True, False,  True, False, False,  True, False, False,\n",
      "        False,  True,  True,  True,  True, False,  True, False, False,  True])\n",
      "Compressed results QSGD values: tensor([ 50, 154, 179,  18,  78,  29,  40,  30, 158, 188,  67,  24, 224,  40,\n",
      "        226,  76, 119,  91,  70, 102,  66, 239,  37,  97,  74, 207,  88,   8,\n",
      "         48, 127,  35,  20,  73,  64,  20,  16,  76, 186, 119, 156,   3,  87,\n",
      "        190,  83, 115,  40,   2,  41,  21,  47,  24,  10,  25,  59, 115, 227,\n",
      "         80, 156,  99,   8, 118,  83, 109,  43,  68,  51,  61, 164,  90,  80,\n",
      "         13, 181,  10,   8,  55,  41,   2,  98, 134,  45,  52,  55, 122,  58,\n",
      "         53, 256, 122, 155,   4,  91,  75,  26,  39,  73, 108, 135,  18, 176,\n",
      "        140,   7], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# qsgd\n",
    "tensor = torch.randn(size=(100,))\n",
    "shape = tensor.shape\n",
    "print(\"To be compressed tensor:\", tensor)\n",
    "\n",
    "# compress\n",
    "norm, signs, values = qsgd_compressor.compress(tensor)\n",
    "print(\"Compressed results QSGD norm:\", norm)\n",
    "print(\"Compressed results QSGD signs:\", signs)\n",
    "print(\"Compressed results QSGD values:\", values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompressed results: tensor([ 0.4978,  1.5331, -1.7820, -0.1792, -0.7765,  0.2887, -0.3982,  0.2987,\n",
      "         1.5730,  1.8716, -0.6670, -0.2389, -2.2300,  0.3982,  2.2499,  0.7566,\n",
      "         1.1847,  0.9060, -0.6969, -1.0155,  0.6571, -2.3794, -0.3684,  0.9657,\n",
      "         0.7367,  2.0608,  0.8761, -0.0796,  0.4779, -1.2643,  0.3484, -0.1991,\n",
      "        -0.7268,  0.6372, -0.1991, -0.1593, -0.7566,  1.8517,  1.1847, -1.5531,\n",
      "         0.0299, -0.8661,  1.8915, -0.8263, -1.1449,  0.3982, -0.0199,  0.4082,\n",
      "         0.2091,  0.4679,  0.2389,  0.0996,  0.2489, -0.5874, -1.1449, -2.2599,\n",
      "        -0.7964,  1.5531,  0.9856,  0.0796, -1.1747,  0.8263, -1.0851,  0.4281,\n",
      "         0.6770, -0.5077,  0.6073, -1.6327, -0.8960, -0.7964, -0.1294, -1.8019,\n",
      "         0.0996, -0.0796,  0.5476, -0.4082, -0.0199,  0.9756, -1.3340, -0.4480,\n",
      "        -0.5177, -0.5476,  1.2146, -0.5774,  0.5276, -2.5486, -1.2146,  1.5431,\n",
      "        -0.0398, -0.9060, -0.7467,  0.2588,  0.3883,  0.7268,  1.0752, -1.3440,\n",
      "         0.1792, -1.7522, -1.3938,  0.0697])\n"
     ]
    }
   ],
   "source": [
    "# decompress\n",
    "decompressed = qsgd_compressor.decompress([norm, signs, values])\n",
    "print(\"Decompressed results:\", decompressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use compressor in federated learning\n",
    "\n",
    "For example on the client side, we could compress the tensors are to compressed and upload the compressed results to server. And server could decompress the tensors follows the compression agreements.\n",
    "\n",
    "In jupyter notebook, we take the standalone scenario as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlab.contrib.algorithm.basic_client import SGDSerialClientTrainer, SGDClientTrainer\n",
    "from fedlab.contrib.algorithm.basic_server import SyncServerHandler\n",
    "\n",
    "class TopkSerialClientTrainer(SGDSerialClientTrainer):\n",
    "    def setup_compressor(self, k):\n",
    "        self.compressor = TopkCompressor(compress_ratio=k)\n",
    "\n",
    "    @property\n",
    "    def uplink_package(self):\n",
    "        package = super().uplink_package\n",
    "        new_package = []\n",
    "        for content in package:\n",
    "            pack = [self.compressor.compress(content[0])]\n",
    "            new_package.append(pack)\n",
    "        return new_package\n",
    "\n",
    "class TopkServerHandeler(SyncServerHandler):\n",
    "    def setup_compressor(self, k):\n",
    "        self.compressor = TopkCompressor(compress_ratio=k)\n",
    "\n",
    "    def load(self, payload) -> bool:\n",
    "        values, indices = payload[0]\n",
    "        decompressed_payload = self.compressor.decompress(values, indices, self.model_parameters.shape)\n",
    "        return super().load(decompressed_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main, this part we follow the pipeline in pipeline_tutorial.ipynb\n",
    "# But replace the hander and trainer by the above defined for communication compression\n",
    "\n",
    "# configuration\n",
    "from munch import Munch\n",
    "from fedlab.models.mlp import MLP\n",
    "\n",
    "model = MLP(784, 10)\n",
    "args = Munch\n",
    "\n",
    "args.total_client = 100\n",
    "args.alpha = 0.5\n",
    "args.seed = 42\n",
    "args.preprocess = False\n",
    "args.cuda = True\n",
    "\n",
    "from torchvision import transforms\n",
    "from fedlab.contrib.dataset.partitioned_mnist import PartitionedMNIST\n",
    "\n",
    "fed_mnist = PartitionedMNIST(root=\"./tests/data/mnist/\",\n",
    "                             path=\"./tests/data/mnist/fedmnist/\",\n",
    "                             num_clients=args.total_client,\n",
    "                             partition=\"noniid-labeldir\",\n",
    "                             dir_alpha=args.alpha,\n",
    "                             seed=args.seed,\n",
    "                             preprocess=args.preprocess,\n",
    "                             download=True,\n",
    "                             verbose=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToPILImage(),\n",
    "                                 transforms.ToTensor()\n",
    "                             ]))\n",
    "\n",
    "dataset = fed_mnist.get_dataset(0)  # get the 0-th client's dataset\n",
    "dataloader = fed_mnist.get_dataloader(\n",
    "    0,\n",
    "    batch_size=128)  # get the 0-th client's dataset loader with batch size 128\n",
    "\n",
    "# client\n",
    "from fedlab.contrib.algorithm.basic_client import SGDSerialClientTrainer, SGDClientTrainer\n",
    "\n",
    "# local train configuration\n",
    "args.epochs = 5\n",
    "args.batch_size = 128\n",
    "args.lr = 0.1\n",
    "\n",
    "trainer = TopkSerialClientTrainer(model, args.total_client,\n",
    "                                 cuda=args.cuda)  # serial trainer\n",
    "# trainer = SGDClientTrainer(model, cuda=True) # single trainer\n",
    "\n",
    "trainer.setup_dataset(fed_mnist)\n",
    "trainer.setup_optim(args.epochs, args.batch_size, args.lr)\n",
    "\n",
    "\n",
    "# server\n",
    "from fedlab.contrib.algorithm.basic_server import SyncServerHandler\n",
    "\n",
    "# global configuration\n",
    "args.com_round = 10\n",
    "args.sample_ratio = 0.1\n",
    "\n",
    "handler = TopkServerHandeler(model=model,\n",
    "                            global_round=args.com_round,\n",
    "                            sample_ratio=args.sample_ratio,\n",
    "                            cuda=args.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedlab.utils.functional import evaluate\n",
    "from fedlab.core.standalone import StandalonePipeline\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class EvalPipeline(StandalonePipeline):\n",
    "    def __init__(self, handler, trainer, test_loader):\n",
    "        super().__init__(handler, trainer)\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "    def main(self):\n",
    "        while self.handler.if_stop is False:\n",
    "            # server side\n",
    "            sampled_clients = self.handler.sample_clients()\n",
    "            broadcast = self.handler.downlink_package\n",
    "\n",
    "            # client side\n",
    "            self.trainer.local_process(broadcast, sampled_clients)\n",
    "            uploads = self.trainer.uplink_package\n",
    "\n",
    "            # server side\n",
    "            for pack in uploads:\n",
    "                self.handler.load(pack)\n",
    "\n",
    "            loss, acc = evaluate(self.handler.model, nn.CrossEntropyLoss(),\n",
    "                                 self.test_loader)\n",
    "            print(\"loss {:.4f}, test accuracy {:.4f}\".format(loss, acc))\n",
    "\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root=\"./tests/data/mnist/\",\n",
    "                                       train=False,\n",
    "                                       transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_data, batch_size=1024)\n",
    "\n",
    "standalone_eval = EvalPipeline(handler=handler,\n",
    "                               trainer=trainer,\n",
    "                               test_loader=test_loader)\n",
    "standalone_eval.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('fedlab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "019ae50596e3d4df627f3288be8543f4b17347150bdb9d2aa2e7c637014aee00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
